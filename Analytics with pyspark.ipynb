{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"project1\").getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"nullvalue\",\"NA\").option(\"escape\",\"\\\"\").option(\"path\",\"fake_job_postings.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(subset='salary_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "mini=udf(lambda x : x.partition('-')[0])\n",
    "maxi=udf(lambda x : x.partition('-')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('min',mini(df.salary_range))\n",
    "df = df.withColumn('max',maxi(df.salary_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting min and max columns to integer types\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn('min',df[\"min\"].cast('int'))\n",
    "df = df.withColumn('max',df[\"max\"].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake=df.select(\"salary_range\",\"min\",\"max\").filter(\"fraudulent==1\")\n",
    "df_real=df.select(\"salary_range\",\"min\",\"max\").filter(\"fraudulent==0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_median = df_fake.approxQuantile('min',[0.5],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1 : computing the mean and standard deviation of fake job postings\n",
    "df_fake.describe('max').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question2\n",
    "approxmedian_rj = df_real.approxQuantile('min',[0.5],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approxmedian_rj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "w = Window.orderBy('min')\n",
    "df_real = df_real.withColumn('id',row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mid_id\n",
    "round(df_real.count()/2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i have identified the outliers to be seven figure salaries, dates and three figure salaries\n",
    "df_real.select('min').filter('id==1323').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers\n",
    "#removing dates\n",
    "df=df.dropna(subset=['min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.filter('min>1000'and 'max>1000')\n",
    "df1 = df1.filter('min<500000'and 'max<500000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake1=df1.select(\"salary_range\",\"min\",\"max\").filter(\"fraudulent==1\")\n",
    "df_real1=df1.select(\"salary_range\",\"min\",\"max\").filter(\"fraudulent==0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake1.describe('max').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question2\n",
    "approxmedian_rjn = df_real1.approxQuantile('min',[0.5],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approxmedian_rjn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real1.count()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real1 = df_real1.withColumn('id',row_number().over(w))\n",
    "df_real1.select('min').filter('id==1119').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn=df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"nullvalue\",\"NA\").option(\"escape\",\"\\\"\").option(\"path\",\"fake_job_postings.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_real=dfn.select('description','requirements').filter('fraudulent==0').fillna(' ').rdd.map(tuple)\n",
    "rdd_fake=dfn.select('description','requirements').filter('fraudulent==1').fillna(' ').rdd.map(tuple)\n",
    "rdd_real = rdd_real.map(lambda x: x[0]+x[1])\n",
    "rdd_fake = rdd_fake.map(lambda x: x[0]+x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=dfn.select('description','requirements').fillna(' ').rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: x[0]+x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minor text cleaning\n",
    "import re\n",
    "rdd_real = rdd_real.map(lambda x : re.sub('[^a-zA-Z]',' ',x))#keeping just the alphabets\n",
    "rdd_real = rdd_real.map(lambda x : x.lower())\n",
    "rdd_fake = rdd_fake.map(lambda x : re.sub('[^a-zA-Z]',' ',x))#keeping just the alphabets\n",
    "rdd_fake = rdd_fake.map(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_fake.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_func(line):\n",
    "    bigrams = []\n",
    "    unigrams = line.strip().split()\n",
    "    for i in range(len(unigrams) - 1):\n",
    "        bigram = unigrams[i] + \":\" + unigrams[i+1]\n",
    "        bigrams.append(bigram)\n",
    "\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_func(line):\n",
    "    trigrams = []\n",
    "    unigrams = line.strip().split()\n",
    "    for i in range(len(unigrams) - 2):\n",
    "        trigram = unigrams[i] + \":\" + unigrams[i+1] + \":\" + unigrams[i+2]\n",
    "        trigrams.append(trigram)\n",
    "\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "#computing bigrams for the real postings\n",
    "bcount_real = rdd_real.flatMap(bigrams_func).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "bcount_real.sortBy(lambda x: x[1],ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams for fake postings\n",
    "bcount_fake = rdd_fake.flatMap(bigrams_func).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "bcount_fake.sortBy(lambda x: x[1],ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing trigrams for real job job postings\n",
    "tcount_real = rdd_real.flatMap(trigrams_func).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "tcount_real.sortBy(lambda x: x[1],ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing trigrams for fake job postings\n",
    "tcount_fake = rdd_fake.flatMap(trigrams_func).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "tcount_fake.sortBy(lambda x: x[1],ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
